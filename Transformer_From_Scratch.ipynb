{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pesyfrR24BJW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import os\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gWZkwwFG4QZP"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class configs:\n",
        "    num_epochs: int = 10\n",
        "    batch_size: int = 64\n",
        "    learning_rate: float = 0.001\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JrkvziHVjDwb"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class model_configs:\n",
        "    src_n_vocab: int = 10_000\n",
        "    target_n_vocab: int = 5_000\n",
        "    # max_len: int = 512\n",
        "    d_model: int = 512\n",
        "    num_layers: int = 6\n",
        "    num_heads: int = 8\n",
        "    dropout: float = 0.1\n",
        "    d_k: int = d_model // num_heads # 512 / 8 = 64 ## to be able to project it BY W_o --> h * d_k x d_model\n",
        "    d_ff: int = 2048\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RQ6ij4-S4ZmD"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, n_vocab, d_model):\n",
        "        super(InputEmbedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(n_vocab, d_model)\n",
        "\n",
        "        # self.dropout = nn.Dropout(configs.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x.int()) * self.d_model ** 0.5\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jF2sK8oc43Bv"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        i_vector = torch.arange(0, d_model, 2, dtype=torch.float)\n",
        "\n",
        "        denom = torch.pow(10_000.0, i_vector / d_model)\n",
        "\n",
        "        pe[:, ::2] = torch.sin(pos / denom)\n",
        "        pe[:, 1::2] = torch.sin(pos / denom)\n",
        "\n",
        "        pe = pe.unsqueeze(0) # add batch dim\n",
        "\n",
        "        self.register_buffer('pe', pe) # register buffer to be saved in the model state_dict\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return x + self.pe[:, :x.size(1)].requires_grad_(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I1aHArOS47q2"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, eps: float = 1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.tensor(1.), requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.tensor(1.), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "\n",
        "        return self.weight * (x - mean) / torch.sqrt(std ** 2 + self.eps) + self.bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vZ-l_1fixg2i"
      },
      "outputs": [],
      "source": [
        "class Add_and_Norm(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(Add_and_Norm, self).__init__()\n",
        "        self.layer_norm = LayerNorm()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, sublayer_output):\n",
        "        return self.layer_norm(x + self.dropout(sublayer_output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Nhae5upXxg2j"
      },
      "outputs": [],
      "source": [
        "class selfAttentionHead(nn.Module):\n",
        "    def __init__(self, d_model, d_k):\n",
        "        super(selfAttentionHead, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_k)\n",
        "        self.wk = nn.Linear(d_model, d_k)\n",
        "        self.wv = nn.Linear(d_model, d_k)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        q = self.wq(x) # shape: (bs, N, d_k)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "\n",
        "        attention_matrix = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5) # (bs, N, N)\n",
        "        attention_scores = torch.softmax(attention_matrix, dim=-1)\n",
        "\n",
        "        weighted_value = torch.matmul(attention_scores, v)\n",
        "\n",
        "        return weighted_value # (bs, N, d_k)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3iRkvDq-5Ap6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    '''\n",
        "    MultiHead Attention implementation with parallel heads\n",
        "\n",
        "    '''\n",
        "    def __init__(self, num_heads, d_k, causal=False):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        d_model = num_heads * d_k\n",
        "\n",
        "        self.d_k = d_k\n",
        "\n",
        "        self.wq = nn.Linear(d_model, d_model) # (d_model, d_k * num_heads)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.wo = nn.Linear(d_model, d_model) # (num_heads * d_k, d_model) = (d_model, d_model)\n",
        "\n",
        "        self.causal = causal\n",
        "\n",
        "    def attention(self, q, k, v):\n",
        "\n",
        "        attention_matrix = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5) # (bs, num_heads, N, N)\n",
        "\n",
        "        if self.causal:\n",
        "            mask = torch.tril(torch.ones(attention_matrix.shape[-2], attention_matrix.shape[-1])).unsqueeze(0).unsqueeze(0).to(q.device) # (1, 1, N, N)\n",
        "            attention_matrix = attention_matrix.masked_fill_(mask == 0, value= float('-inf'))\n",
        "\n",
        "\n",
        "        attention_scores = torch.softmax(attention_matrix, dim=-1)\n",
        "\n",
        "        weighted_value = torch.matmul(attention_scores, v) # (bs, num_heads, q_seq_len, d_k)\n",
        "\n",
        "        return weighted_value, attention_scores\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "\n",
        "\n",
        "\n",
        "        q = self.wq(q) # shape: (bs, seq_len, d_model)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        ## split heads\n",
        "        q = q.view(-1, q.shape[1], self.num_heads, self.d_k).transpose(1, 2).contiguous() # (bs, h, seq_len, d_k)\n",
        "        k = k.view(-1, k.shape[1], self.num_heads, self.d_k).transpose(1, 2).contiguous()\n",
        "        v = v.view(-1, v.shape[1], self.num_heads, self.d_k).transpose(1, 2).contiguous()\n",
        "\n",
        "\n",
        "        weighted_value, self.attention_scores = self.attention(q, k, v)\n",
        "\n",
        "\n",
        "        ##  concatenate all heads together\n",
        "        weighted_value = weighted_value.transpose(1, 2).contiguous().view(-1, q.shape[2], d_model)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        output = self.wo(weighted_value) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        return output # (bs, seq_len, d_model), (bs, seq_len, seq_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vI8f82DS5LDi"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear_lift = nn.Linear(d_model, d_ff)\n",
        "        self.linear_out = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.relu(self.linear_lift(x))\n",
        "      x = self.dropout(x)\n",
        "      x = self.linear_out(x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QT6BurFXxg2l"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int = 512, d_k: int = 64, num_heads: int = 8, d_ff: int=2048, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.multiheadAttention = MultiHeadAttention(num_heads=num_heads, d_k=d_k)\n",
        "        self.add_and_norm = Add_and_Norm(dropout)\n",
        "        self.ffn = FeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      shortcut = x\n",
        "      x = self.multiheadAttention(x, x, x) # self-attention\n",
        "      x = self.add_and_norm(shortcut, x)\n",
        "\n",
        "      shortcut = x\n",
        "      x = self.ffn(x)\n",
        "      x = self.add_and_norm(shortcut, x)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GlHuxoNI5cf1"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers: int = 6, d_model: int = 512, d_k: int = 64, num_heads: int = 8, d_ff: int=2048):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "                        EncoderLayer(d_model, d_k, num_heads, d_ff) for _ in range(num_layers)\n",
        "                        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IcryKm4Kxg2n"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int = 512, d_k: int = 64, num_heads: int = 8, d_ff: int=2048, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.multiheadcrossAttention = MultiHeadAttention(num_heads=num_heads, d_k=d_k)\n",
        "        self.causal_multiheadAttention = MultiHeadAttention(num_heads=num_heads, d_k=d_k, causal=True)\n",
        "        self.add_and_norm = Add_and_Norm(dropout)\n",
        "        self.ffn = FeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.causal_multiheadAttention(x, x, x) # self-attention\n",
        "        x = self.add_and_norm(shortcut, x)\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.multiheadcrossAttention(x, encoder_output, encoder_output) # cross-attention\n",
        "        x = self.add_and_norm(shortcut, x)\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.add_and_norm(shortcut, x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "j-MjCrwaxg2n"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_vocab: int = 1000, num_layers: int = 6, d_model: int = 512, d_k: int = 64, num_heads: int = 8, d_ff: int=2048):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "                        DecoderLayer(d_model, d_k, num_heads, d_ff) for _ in range(num_layers)\n",
        "                        ])\n",
        "\n",
        "    def forward(self, x, encoder_output):\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ejCqw0XK5li1"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.src_embed = InputEmbedding(n_vocab=configs.src_n_vocab, d_model=configs.d_model)\n",
        "        self.target_embed = InputEmbedding(n_vocab=configs.target_n_vocab, d_model=configs.d_model)\n",
        "\n",
        "        self.pos_embed = PositionalEncoding(d_model=configs.d_model)\n",
        "\n",
        "        self.encoder = Encoder(num_layers=configs.num_layers, d_model=configs.d_model,\n",
        "                                d_k=configs.d_k, num_heads=configs.num_heads, d_ff=configs.d_ff)\n",
        "\n",
        "        self.decoder = Decoder(num_layers=configs.num_layers, d_model=configs.d_model,\n",
        "                                d_k=configs.d_k, num_heads=configs.num_heads, d_ff=configs.d_ff)\n",
        "\n",
        "        self.projection_layer = nn.Sequential(nn.Linear(configs.d_model, configs.target_n_vocab),\n",
        "                                          nn.Softmax(dim=-1)\n",
        "                                        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def embed_src(self, x):\n",
        "        x = self.src_embed(x)\n",
        "        x = self.pos_embed(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def embed_tgt(self, x):\n",
        "        x = self.target_embed(x)\n",
        "        x = self.pos_embed(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_weights(self):\n",
        "        for param in self.parameters():\n",
        "            if param.dim() > 1:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def generate(self, src, max_len, start_token, end_token):\n",
        "        '''\n",
        "        inference time autoregressive generation\n",
        "        '''\n",
        "\n",
        "        src = self.embed_src(src)\n",
        "\n",
        "        encoder_output = self.encoder(src)\n",
        "        generated_sequence = [start_token]\n",
        "\n",
        "\n",
        "\n",
        "        for _ in range(max_len):\n",
        "\n",
        "            target = torch.tensor(generated_sequence).unsqueeze(0).to(src.device)\n",
        "            target = self.embed_tgt(target)\n",
        "\n",
        "            decoder_output = self.decoder(target, encoder_output)\n",
        "            logits = self.projection_layer(decoder_output[:, -1, :])\n",
        "            next_token = torch.argmax(logits, dim=-1).item()\n",
        "            generated_sequence.append(next_token)\n",
        "\n",
        "            if next_token == end_token:\n",
        "                break\n",
        "\n",
        "        return torch.tensor(generated_sequence)\n",
        "\n",
        "    def forward(self, src, target):\n",
        "\n",
        "        src = self.embed_src(src)\n",
        "        target = self.embed_tgt(target)\n",
        "\n",
        "        encoder_output = self.encoder(src)\n",
        "        decoder_output = self.decoder(target, encoder_output)\n",
        "\n",
        "        output = self.projection_layer(decoder_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "E2hkmvvcxg2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d6055b-4cd4-455f-9790-cc377037fb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output:  torch.Size([1, 17, 5000])\n",
            "torch.Size([21])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "batch_size = 1\n",
        "seq_length = 10\n",
        "src_n_vocab = 10000\n",
        "target_n_vocab = 5000\n",
        "d_model = 512\n",
        "\n",
        "src = torch.randint(0, src_n_vocab, (batch_size, seq_length))  # Source sequence\n",
        "target = torch.randint(0, target_n_vocab, (batch_size, seq_length + 7))  # Source sequence\n",
        "\n",
        "start_token = 1\n",
        "end_token = 2\n",
        "max_length = 20\n",
        "\n",
        "model = Transformer(configs=model_configs)\n",
        "\n",
        "output = model.forward(src, target)\n",
        "\n",
        "print(\"Transformer output: \", output.shape)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "generated_sequence = model.generate(src, max_length, start_token, end_token)\n",
        "print(generated_sequence.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Udhy_deHxg2p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aYPQ5U4O6Df_"
      },
      "outputs": [],
      "source": [
        "|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOVTmFUHxg2p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InUgCRShxg2p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}